{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049d39bf",
   "metadata": {},
   "source": [
    "## Create index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59479543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import struct\n",
    "\n",
    "os.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"upb\")\n",
    "\n",
    "DATA_ROOT = '/path/to/your/tfrecords'\n",
    "INDEX_FILE = 'index.idx'\n",
    "BATCH_SIZE = 32\n",
    "NUM_CAMS = 8\n",
    "\n",
    "train_file_paths = sorted(glob.glob(os.path.join(DATA_ROOT, 'training_*.tfrecord*')))\n",
    "\n",
    "if not os.path.exists(INDEX_FILE):\n",
    "\n",
    "    with open(INDEX_FILE, 'wb') as index_f:\n",
    "        for i, file_path in enumerate(train_file_paths):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                while True:\n",
    "                    proto_size_bytes = f.read(8)\n",
    "                    if not proto_size_bytes:\n",
    "                        break  # EOF\n",
    "\n",
    "                    proto_size = struct.unpack('<Q', proto_size_bytes)[0]\n",
    "\n",
    "                    index_f.write(struct.pack('<Q', i))\n",
    "                    index_f.write(struct.pack('<Q', f.tell() + 4)) # +4 to skip length Checksum\n",
    "                    index_f.write(struct.pack('<Q', proto_size))\n",
    "\n",
    "                    # Move past proto and two checksums\n",
    "                    f.seek(proto_size + 8, os.SEEK_CUR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64fe99",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74caaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "with open(INDEX_FILE, 'rb') as f:\n",
    "    while True:\n",
    "        file_idx_bytes = f.read(8)\n",
    "\n",
    "        if not file_idx_bytes:\n",
    "            break # EOF\n",
    "\n",
    "        file_idx = struct.unpack('<Q', file_idx_bytes)[0]\n",
    "        offset = struct.unpack('<Q', f.read(8))[0]\n",
    "        size = struct.unpack('<Q', f.read(8))[0]\n",
    "        index.append((file_idx, offset, size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978349e",
   "metadata": {},
   "source": [
    "## Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbfdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import IterableDataset, get_worker_info\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2\n",
    "\n",
    "class WaymoDataset(IterableDataset):\n",
    "    def __init__(self, data_root, index):\n",
    "        self.data_root = data_root\n",
    "        self.index = index\n",
    "        self.file_paths = sorted(glob.glob(os.path.join(data_root, 'training_*.tfrecord*')))\n",
    "        self.file = None\n",
    "        self.file_path = None\n",
    "\n",
    "    def _shard_index(self):\n",
    "        wi = get_worker_info()\n",
    "        if wi is None:\n",
    "            return self.index\n",
    "        \n",
    "        return [rec for i, rec in enumerate(self.index) if i % wi.num_workers == wi.id]\n",
    "\n",
    "    def __iter__(self):\n",
    "        shard = self._shard_index()\n",
    "\n",
    "        for file_idx, offset, size in shard:\n",
    "            file_path = self.file_paths[file_idx]\n",
    "            if file_path != self.file_path:\n",
    "                if self.file:\n",
    "                    self.file.close()\n",
    "                \n",
    "                self.file_path = file_path\n",
    "                self.file = open(file_path, 'rb')\n",
    "\n",
    "            self.file.seek(offset)\n",
    "            proto_raw = self.file.read(size)\n",
    "\n",
    "            record = end_to_end_driving_data_pb2.E2EDFrame()\n",
    "            record.ParseFromString(proto_raw)\n",
    "\n",
    "            images = [np.frombuffer(img.image, dtype=np.uint8) for img in record.frame.images]\n",
    "\n",
    "            fs = record.future_states\n",
    "            if fs.pos_x:\n",
    "                px = np.asarray(fs.pos_x, dtype=np.float32)\n",
    "                py = np.asarray(fs.pos_y, dtype=np.float32)\n",
    "                pz = np.asarray(fs.pos_z, dtype=np.float32)\n",
    "                future_states = np.stack([px, py, pz], axis=-1)   # [T, 3]\n",
    "            else:\n",
    "                future_states = np.empty((0,3), dtype=np.float32)\n",
    "\n",
    "            yield images, future_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0d0cd",
   "metadata": {},
   "source": [
    "## Setup pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c21cc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def, types, fn\n",
    "\n",
    "def waymo_generator(dataset):\n",
    "    for images_bufs, future_states in dataset:\n",
    "        if len(images_bufs) != NUM_CAMS:\n",
    "            raise ValueError(f\"Expected {NUM_CAMS} images, but got {len(images_bufs)}\")\n",
    "        yield (*images_bufs, future_states)\n",
    "\n",
    "\n",
    "@pipeline_def\n",
    "def waymo_data_pipe(source):\n",
    "    *image_bufs, future_states = fn.external_source(\n",
    "        source=source,\n",
    "        num_outputs=NUM_CAMS + 1,\n",
    "        batch=False,\n",
    "        device=\"cpu\",\n",
    "        no_copy=False\n",
    "    )\n",
    "\n",
    "    images = [fn.decoders.image(img_buf, device=\"mixed\", output_type=types.RGB, use_fast_idct=True) for img_buf in image_bufs]\n",
    "\n",
    "    return *images, future_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "650b44b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:19<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations per second: 12.432738168611408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = WaymoDataset(DATA_ROOT, index)\n",
    "\n",
    "pipe = waymo_data_pipe(\n",
    "    source=waymo_generator(dataset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_threads=8,\n",
    "    device_id=0,\n",
    "    prefetch_queue_depth=3\n",
    ")\n",
    "\n",
    "pipe.build()\n",
    "\n",
    "num_batches = len(index) // BATCH_SIZE\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(num_batches)):\n",
    "    try:\n",
    "        outputs = pipe.run()\n",
    "        del outputs\n",
    "    except StopIteration:\n",
    "        print(\"Stop Iteration!\")\n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "print(f\"iterations per second: {num_batches / (end - start)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
