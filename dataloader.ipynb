{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ca1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 15/263 [02:22<39:09,  9.47s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         proto_len \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m, blenth)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 24\u001b[0m         \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_len\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m         indexes\u001b[38;5;241m.\u001b[39mappend((fn, file\u001b[38;5;241m.\u001b[39mtell()))\n\u001b[1;32m     27\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "should_index = True\n",
    "if should_index: # Takes approx. 9 mins to index\n",
    "    import os\n",
    "    import mmap\n",
    "    import struct\n",
    "    import time\n",
    "    import pickle\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    DATA_DIR = '/scratch/gilbreth/bnamikas/data/waymo_open_dataset_end_to_end_camera_v_1_0_0'\n",
    "\n",
    "    indexes = []\n",
    "\n",
    "    start = time.time()\n",
    "    for i, fn in enumerate(tqdm([file for file in os.listdir(DATA_DIR) if '.tfrecord' in file and file.startswith('train')])):\n",
    "        with open(os.path.join(DATA_DIR, fn), 'rb') as file:\n",
    "            indexes.append((fn, file.tell()))\n",
    "            while True:\n",
    "                blenth = file.read(8)\n",
    "                if len(blenth) == 0:\n",
    "                    indexes.pop(-1)\n",
    "                    break\n",
    "                proto_len = struct.unpack('q', blenth)[0]\n",
    "                file.read(proto_len+8)\n",
    "\n",
    "                indexes.append((fn, file.tell()))\n",
    "\n",
    "    with open('index.pkl', 'wb') as f:\n",
    "        pickle.dump(indexes, f)\n",
    "\n",
    "    print(\"Done indexing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdf1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PAST': array([[ 9.11132812e-01, -6.79931641e-02,  9.11132812e-01,\n",
       "         -8.95705402e-01,  2.10346878e-02, -2.93274820e-01,\n",
       "         -6.97642565e-04],\n",
       "        [ 7.26562500e-01, -5.49316406e-02,  7.26562500e-01,\n",
       "         -7.96713233e-01,  1.63470209e-02,  9.89921689e-02,\n",
       "         -4.68766689e-03],\n",
       "        [ 5.06835938e-01, -4.22363281e-02,  5.06835938e-01,\n",
       "         -5.84413052e-01,  1.66303962e-02,  2.12300181e-01,\n",
       "          2.83375382e-04],\n",
       "        [ 3.31542969e-01, -3.21044922e-02,  3.31542969e-01,\n",
       "         -3.22308868e-01,  1.12319738e-02,  2.62104183e-01,\n",
       "         -5.39842248e-03],\n",
       "        [ 2.18261719e-01, -2.51464844e-02,  2.18261719e-01,\n",
       "         -7.06311315e-02,  3.87633406e-03,  2.51677752e-01,\n",
       "         -7.35563971e-03],\n",
       "        [ 1.70410156e-01, -2.19726562e-02,  1.70410156e-01,\n",
       "         -2.36727092e-02,  5.51053137e-03,  4.69584242e-02,\n",
       "          1.63419731e-03],\n",
       "        [ 1.70410156e-01, -2.00195312e-02,  1.70410156e-01,\n",
       "          2.31026905e-04,  5.53286169e-03,  2.39037368e-02,\n",
       "          2.23303214e-05],\n",
       "        [ 1.67480469e-01, -1.85546875e-02,  1.67480469e-01,\n",
       "          1.43995846e-03,  4.92360489e-03,  1.20893156e-03,\n",
       "         -6.09256793e-04],\n",
       "        [ 1.64062500e-01, -1.68457031e-02,  1.64062500e-01,\n",
       "         -9.62112099e-03,  3.93328862e-03, -1.10610798e-02,\n",
       "         -9.90316272e-04],\n",
       "        [ 1.63574219e-01, -1.51367188e-02,  1.63574219e-01,\n",
       "         -7.60746840e-03,  7.27008237e-03,  2.01365259e-03,\n",
       "          3.33679374e-03],\n",
       "        [ 1.62109375e-01, -1.33056641e-02,  1.62109375e-01,\n",
       "         -7.03825131e-02,  7.32138939e-03, -6.27750456e-02,\n",
       "          5.13070263e-05],\n",
       "        [ 1.53808594e-01, -1.14746094e-02,  1.53808594e-01,\n",
       "         -1.24832645e-01,  6.71608746e-03, -5.44501320e-02,\n",
       "         -6.05301932e-04],\n",
       "        [ 1.28417969e-01, -8.91113281e-03,  1.28417969e-01,\n",
       "         -1.57040447e-01,  5.12196869e-03, -3.22078019e-02,\n",
       "         -1.59411877e-03],\n",
       "        [ 9.47265625e-02, -6.10351562e-03,  9.47265625e-02,\n",
       "         -1.89198375e-01,  2.71172076e-03, -3.21579278e-02,\n",
       "         -2.41024792e-03],\n",
       "        [ 5.17578125e-02, -3.05175781e-03,  5.17578125e-02,\n",
       "         -2.35911697e-01,  4.65066731e-03, -4.67133224e-02,\n",
       "          1.93894655e-03],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         -2.35911697e-01,  4.65066731e-03, -4.67133224e-02,\n",
       "          1.93894655e-03]]),\n",
       " 'FUTURE': array([[-0.06787109,  0.00366211],\n",
       "        [-0.14990234,  0.0078125 ],\n",
       "        [-0.234375  ,  0.01159668],\n",
       "        [-0.31347656,  0.01513672],\n",
       "        [-0.38134766,  0.01818848],\n",
       "        [-0.43945312,  0.02075195],\n",
       "        [-0.48681641,  0.02319336],\n",
       "        [-0.52197266,  0.02478027],\n",
       "        [-0.53955078,  0.02648926],\n",
       "        [-0.54589844,  0.02770996],\n",
       "        [-0.54931641,  0.02905273],\n",
       "        [-0.55712891,  0.03027344],\n",
       "        [-0.57177734,  0.03100586],\n",
       "        [-0.51367188,  0.03015137],\n",
       "        [-0.37353516,  0.02697754],\n",
       "        [-0.13964844,  0.01953125],\n",
       "        [ 0.19580078,  0.00671387],\n",
       "        [ 0.64501953, -0.01525879],\n",
       "        [ 1.20068359, -0.05432129],\n",
       "        [ 1.85644531, -0.1151123 ]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import protobuf\n",
    "import protos.e2e_pb2 as e2e_pb2\n",
    "import pickle\n",
    "import struct\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "DATA_DIR = '/scratch/gilbreth/bnamikas/data/waymo_open_dataset_end_to_end_camera_v_1_0_0'\n",
    "\n",
    "\n",
    "with open(\"index.pkl\", 'rb') as f:\n",
    "    indexes = pickle.load(f)\n",
    "\n",
    "def load_idx(idx):\n",
    "    frame = e2e_pb2.E2EDFrame()\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, indexes[idx][0]), 'rb') as f:\n",
    "        blenth = f.read(8)\n",
    "        proto_len = struct.unpack('q', blenth)[0]\n",
    "        f.read(4)\n",
    "        protobuff = f.read(proto_len)\n",
    "        server = frame.ParseFromString(protobuff)\n",
    "\n",
    "    print(len(frame.past_states.pos_z))\n",
    "\n",
    "    past = np.stack([frame.past_states.pos_x, frame.past_states.pos_y, frame.past_states.vel_x, frame.past_states.vel_y, frame.past_states.accel_x, frame.past_states.accel_y], axis=-1)\n",
    "    \n",
    "\n",
    "    future = np.stack([frame.future_states.pos_x, frame.future_states.pos_y], axis=-1)\n",
    "\n",
    "\n",
    "    return {'PAST':past, 'FUTURE':future}#frame.intent#np.array(list(zip(frame.future_states.pos_x, frame.future_states.pos_y, frame.future_states.pos_z)))\n",
    "\n",
    "\n",
    "\n",
    "load_idx(99999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bda809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from protos import e2e_pb2\n",
    "import pickle\n",
    "import struct\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO \n",
    "\n",
    "class WaymoE2E(Dataset): \n",
    "    def __init__(self, indexFile = 'index.pkl', data_dir='./dataset'):\n",
    "        self.DATA_DIR = data_dir\n",
    "\n",
    "        with open(indexFile, 'rb') as f:\n",
    "            self.indexes = pickle.load(f)\n",
    "\n",
    "    def decode_img(self, img):\n",
    "        return np.array(Image.open(BytesIO(img)))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = e2e_pb2.E2EDFrame()\n",
    "\n",
    "        with open(os.path.join(self.DATA_DIR, self.indexes[idx][0]), 'rb') as f:\n",
    "            # pass\n",
    "            f.seek(self.indexes[idx][1])\n",
    "            blenth = f.read(8)\n",
    "            proto_len = struct.unpack('q', blenth)[0]\n",
    "            f.read(4)\n",
    "            protobuff = f.read(proto_len)\n",
    "            frame.ParseFromString(protobuff)\n",
    "\n",
    "        return np.vstack([self.decode_img(images.image) for images in frame.frame.images]), np.array(list(zip(frame.future_states.pos_x, frame.future_states.pos_y, frame.future_states.pos_z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22641550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/12990 [02:56<70:50:55, 19.65s/it] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1072405) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 1072405) is killed by signal: Killed. ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      9\u001b[0m     dataset, \n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     11\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     12\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_of_frames \u001b[38;5;129;01min\u001b[39;00m tqdm(loader):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/2025.06-py313/python3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1072405) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "DATA_DIR = '/scratch/gilbreth/bnamikas/data/waymo_open_dataset_end_to_end_camera_v_1_0_0'\n",
    "\n",
    "dataset = WaymoE2E(data_dir = DATA_DIR)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "for batch_of_frames in tqdm(loader):\n",
    "    pass\n",
    "print(\"Total Time:\", time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00dde80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
